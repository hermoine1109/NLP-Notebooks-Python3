{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import time\n",
    "import random\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tokenList):\n",
    "    i=0\n",
    "    for word1 in tokenList:\n",
    "    #conversion into lowercase\n",
    "        word1=word1.lower()\n",
    "    #Takes Care of Multiple Punctuation Marks\n",
    "        word1=word1.replace('.','').replace(',','').replace(':','').replace(';','').replace('!','').replace('?','').replace('(','').replace(')','').replace('-','').replace('_','').replace('\\\\',' ').replace('\\\"',' ').replace('\\'',' ')      \n",
    "    \n",
    "        tokenList[i]=word1        \n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating unigram,bigram,trigram,quadgram dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigramize(unigrams):\n",
    "    od=Counter()\n",
    "    for item in unigrams:\n",
    "        od[item]+=1\n",
    "    return od\n",
    "\n",
    "def bigramize(bigrams):\n",
    "    od2=Counter()\n",
    "    for item in bigrams:\n",
    "        od2[item]+=1\n",
    "    return od2\n",
    "\n",
    "def trigramize(trigrams):\n",
    "    od3=Counter()\n",
    "    for item in trigrams:\n",
    "        od3[item]+=1\n",
    "    return od3\n",
    "\n",
    "def quadgramize(quadgrams):\n",
    "    od4=Counter()\n",
    "    for item in quadgrams:\n",
    "        od4[item]+=1\n",
    "    return od4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the language model\n",
    "Performing necessary modifications and training the langugage model with the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def i_unigrams(unigrams,unigram):\n",
    "    for uni in unigrams:\n",
    "        stri = ''.join(uni)\n",
    "        if not unigram or stri not in unigram:\n",
    "            unigram[stri] = 1;\n",
    "        else:\n",
    "            unigram[stri] = unigram[stri] + 1\n",
    "            \n",
    "def i_bigrams(bigrams,bigram):\n",
    "    for bi in bigrams:\n",
    "        stri = ' '.join(bi)\n",
    "        if not bigram or stri not in bigram:\n",
    "            bigram[stri] = 1;\n",
    "        else:\n",
    "            bigram[stri] = bigram[stri] + 1\n",
    "            \n",
    "\n",
    "def i_trigrams(trigrams,trigram):\n",
    "    for tri in trigrams:\n",
    "        stri = ' '.join(tri)\n",
    "        if not trigram or stri not in trigram:\n",
    "            trigram[stri] = 1;\n",
    "        else:\n",
    "            trigram[stri] = trigram[stri] + 1\n",
    "            \n",
    "def i_quadgrams(quadgrams,quadgram):\n",
    "    for quad in quadgrams:\n",
    "        stri = ' '.join(quad)\n",
    "        if not quadgram or stri not in quadgram:\n",
    "            quadgram[stri] = 1;\n",
    "        else:\n",
    "            quadgram[stri] = quadgram[stri] + 1\n",
    "            \n",
    "def create_token_list(tokens,vocab):\n",
    "    newlist=[]\n",
    "    for word in tokens:\n",
    "        if word:\n",
    "            if not vocab or word not in vocab: \n",
    "                vocab[word] = len(vocab)\n",
    "            newlist.append(str(vocab[word]))\n",
    "    return newlist\n",
    "\n",
    "def remove_punc(text):\n",
    "    for c in string.punctuation:\n",
    "        if c!='\\'':\n",
    "            text=text.replace(c,\" \")\n",
    "    text = text.replace(\"' \",\" \")\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    answer = text.lower()\n",
    "    return answer\n",
    "\n",
    "def tokenize(text, ngrams,vocab):\n",
    "    clean_text = remove_punc(text)\n",
    "    tokens = create_token_list(clean_text.split(),vocab)\n",
    "    #print(tokens)\n",
    "    return [tuple(tokens[i:i+ngrams]) for i in range(len(tokens)-ngrams+1)]\n",
    "\n",
    "def train_data(unigram,bigram,trigram,quadgram,vocab):\n",
    "    l = 0\n",
    "    with open('training_corpus.txt',buffering=20000,encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            data = tokenize(line,1,vocab)\n",
    "            i_unigrams(data,unigram)\n",
    "            l = l + len(data)\n",
    "            data = tokenize(line,2,vocab)\n",
    "            i_bigrams(data,bigram)\n",
    "            data = tokenize(line,3,vocab)\n",
    "            i_trigrams(data,trigram)\n",
    "            data = tokenize(line,4,vocab)\n",
    "            i_quadgrams(data,quadgram)\n",
    "    f.close()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating probability dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bi_probability(unigram,bigram,bi_probab):\n",
    "    #creating bigram probability table\n",
    "    for bi in bigram:\n",
    "        s = bi.split()\n",
    "        w = s[1]\n",
    "        stri = s[0]\n",
    "        p = float(bigram[bi])/unigram[stri]\n",
    "        if stri in bi_probab:\n",
    "            if w not in bi_probab[stri]:\n",
    "                bi_probab[stri][w] = p\n",
    "                d = OrderedDict(sorted(bi_probab[stri].items(), key=lambda t: t[1]))\n",
    "                bi_probab[stri] = d\n",
    "        else:\n",
    "            bi_probab[stri] = {}\n",
    "            bi_probab[stri][w] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tri_probability(bigram,trigram,tri_probab):\n",
    "    #creating trigram probability table\n",
    "    for tri in trigram:\n",
    "        s = tri.split()\n",
    "        w = s[-1]\n",
    "        stri = ' '.join(s[0:2])\n",
    "        p = float(trigram[tri])/bigram[stri]\n",
    "        if stri in tri_probab:\n",
    "            if w not in tri_probab[stri]:\n",
    "                tri_probab[stri][w] = p\n",
    "                d = OrderedDict(sorted(tri_probab[stri].items(), key=lambda t: t[1]))\n",
    "                tri_probab[stri] = d\n",
    "        else:\n",
    "            tri_probab[stri] = {}\n",
    "            tri_probab[stri][w] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quad_probability(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,probab,tokens):\n",
    "    \n",
    "    #getting the parameters of interpolation by performing grid search\n",
    "    l = grid_search(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,tokens)\n",
    "    #lambda1 = 0.25\n",
    "    #lambda2 = 0.25\n",
    "    #lambda3 = 0.25\n",
    "    #lambda4 = 0.25\n",
    "    lambda1 = l[0]\n",
    "    lambda2 = l[1]\n",
    "    lambda3 = l[2]\n",
    "    lambda4 = l[3]\n",
    "    #quadgram probability to be found out after interpolation\n",
    "    for quad in quadgram:\n",
    "        s = quad.split()\n",
    "        w = s[-1]\n",
    "        stri = ' '.join(s[0:3])\n",
    "        p = interpolation(quad,stri,s,quadgram,trigram,bigram,unigram,tokens,lambda1,lambda2,lambda3,lambda4)\n",
    "        if stri in probab:\n",
    "            if w not in probab[stri]:\n",
    "                probab[stri][w] = p\n",
    "                d = OrderedDict(sorted(probab[stri].items(), key=lambda t: t[1]))\n",
    "                probab[stri] = d\n",
    "        else:\n",
    "            probab[stri] = {}\n",
    "            probab[stri][w] = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating quadgram probability table and predicting the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quad_prob(trigrams,quadgrams):\n",
    "    \n",
    "    tri_freq=Counter(trigrams)\n",
    "    quad_freq=Counter(quadgrams)\n",
    "    #print (tri_freq.items())\n",
    "    #creating the quadram probabilty table for each word given its trigram context\n",
    "    for item in quad_freq:\n",
    "        tri=item[0:3]\n",
    "        quad_freq[item]=(quad_freq[item]/tri_freq[tri])\n",
    "        \n",
    "    return quad_freq\n",
    "\n",
    "def pred_table(quad_prob_table):\n",
    "    quad_pred_table=defaultdict(dict)\n",
    "    #creating the prediction table with all the probable words after a given trigram\n",
    "    for quad in quad_prob_table:\n",
    "        prob=quad_prob_table[quad]\n",
    "        tri=quad[0:3]\n",
    "        token=quad[3]\n",
    "        quad_pred_table[tri][token]=prob\n",
    "    #sorting so tht we can find the most probable word in O(1) time     \n",
    "    for tri in quad_pred_table:\n",
    "        quad_pred_table[tri]=sorted(quad_pred_table[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return quad_pred_table\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Interpolation using given weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolation_table(od,od2,od3,od4):\n",
    "    \n",
    "    pole=defaultdict(dict)\n",
    "    \n",
    "    i=0\n",
    "    s=0.0\n",
    "    lambda1=0.25\n",
    "    lambda2=0.25\n",
    "    lambda3=0.25\n",
    "    lambda4=0.25\n",
    "    \n",
    "    #Simple interpolation using given weights\n",
    "    for item,value in od4.items():\n",
    "        #if i>100:\n",
    "         #   break    \n",
    "        p=lambda1*float((value)/od3[item[0:3]]) + lambda2*float((od3[item[1:4]])/(od2[item[1:3]])) + lambda3*float((od2[item[2:4]])/(od[item[2]]+1)) +lambda4*(float((od[item[3]]+1)))\n",
    "        pole[item[0:3]][item[3]]=p    \n",
    "        \n",
    "    for tri in pole:\n",
    "        pole[tri]=sorted(pole[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation using customised weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cus_interpolation_table(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,tokens):\n",
    "\n",
    "    pole=defaultdict(dict)\n",
    "    pmax=0\n",
    "    i=0\n",
    "                         \n",
    "    for item,value in od4.items(): \n",
    "        \n",
    "        for i in range (10):\n",
    "            #finding the weights randomly\n",
    "            #l=weights(l)\n",
    "            #finding the weights by tuning the parameters using grid search\n",
    "            l=grid_search(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,tokens)\n",
    "            p=l[0]*float((value)/od3[item[0:3]]) + l[1]*float((od3[item[1:4]])/(od2[item[1:3]])) + l[2]*float((od2[item[2:4]])/(od[item[2]]+1)) +l[3]*(float((od[item[3]]+1)))\n",
    "            #max probabilty case is considered\n",
    "            if(p>pmax):\n",
    "                pole[item[0:3]][item[3]]=p\n",
    "                pmax=p\n",
    "                                \n",
    "    \n",
    "    for tri in pole:\n",
    "        pole[tri]=sorted(pole[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning testing corpus to create the held-out corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loads the testing corpus and partitions it into equal halves to create testing corpus and held-out corpus\n",
    "def partitionFile(file_path,word_count):\n",
    "    token = []\n",
    "    word_count = int(word_count*0.5)\n",
    "      \n",
    "    pos = 0\n",
    "    word_len = 0\n",
    "\n",
    "    #open the corpus file and read it line by line\n",
    "    file = open(file_path,'r')\n",
    "    test_file = open('testing_corpus.txt','w')\n",
    "    held_file = open('heldset_corpus.txt','w') \n",
    "\n",
    "    line = file.readline()\n",
    "\n",
    "    while line:\n",
    "        #split the line into tokens\n",
    "        token = line.split()\n",
    "\n",
    "        #write the line to the training file\n",
    "        test_file.write(line)\n",
    "\n",
    "        word_len = word_len + len(token)  \n",
    "            \n",
    "        #quit training when 90% of the corpus has been read\n",
    "        if word_len >= word_count:\n",
    "                pos = file.tell()\n",
    "                break;\n",
    "            \n",
    "        line = file.readline()\n",
    "\n",
    "    #Prepare the testing data\n",
    "    if word_count <= word_len:\n",
    "        file.seek(pos)\n",
    "        test_data = file.read();\n",
    "        held_file.write(test_data)\n",
    "            \n",
    "    file.close()\n",
    "    test_file.close()\n",
    "    held_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the weights using grid search\n",
    "\n",
    "We find the weights of interpolation using grid search. For each set of lambda values, we find the score using our held out corpus. The set of lambda values giving us the maximised score is the one which we use for our interpolation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolation(quad,stri,s,quadgram,trigram,bigram,unigram,tokens,lambda1,lambda2,lambda3,lambda4):\n",
    "    \n",
    "    #computing the interpolated probability using all n-grams\n",
    "    p = ( lambda1 * (quadgram[quad]/trigram[stri])) \n",
    "    + (lambda2 * (trigram[' '.join(s[1:4])]/bigram[' '.join(s[1:3])])) \n",
    "    + (lambda3 * (bigram[' '.join(s[2:4])]/unigram[str(s[2])])) \n",
    "    + (lambda4 * (unigram[str(s[3])]/tokens))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def grid_search(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,tokens):\n",
    "    \n",
    "    lambda1 = 0\n",
    "    lambda2 = 0\n",
    "    lambda3 = 0\n",
    "    max_score = 0\n",
    "    i=0\n",
    "    while i<=1:\n",
    "        j=0\n",
    "        while j<=1:\n",
    "            k=0\n",
    "            while k<=1:\n",
    "                if (i+j+k)<=1:\n",
    "                    probab_dict = {}\n",
    "                    for quad in quadgram:\n",
    "                        s = quad.split()\n",
    "                        w = s[-1]\n",
    "                        stri = ' '.join(s[0:3])\n",
    "                        #considering i,j,k,(1-i-j-k) as our lambda values, we find interpolation probabilty\n",
    "                        p = interpolation(quad,stri,s,quadgram,trigram,bigram,unigram,tokens,i,j,k,(1-i-j-k))\n",
    "                        if stri in probab_dict:\n",
    "                            if w not in probab_dict[stri] :\n",
    "                                probab_dict[stri][w] = p\n",
    "                                d = OrderedDict(sorted(probab_dict[stri].items(), key=lambda t: t[1]))\n",
    "                                probab_dict[stri] = d\n",
    "                        else:\n",
    "                            probab_dict[stri] = {}\n",
    "                            probab_dict[stri][w] = p\n",
    "                    #we find out the set of lambda for which the interpolation score is maximun for the held out corpus\n",
    "                    score = interpol_score('held_out_corpus.txt',bi_probab,tri_probab,probab_dict,vocab)\n",
    "                    #print(i,j,k,score,max_score)\n",
    "                    if score > max_score:\n",
    "                        lambda1 = i\n",
    "                        lambda2 = j\n",
    "                        lambda3 = k\n",
    "                        max_score = score\n",
    "                k = k + 0.1\n",
    "            j = j + 0.1\n",
    "        i = i + 0.1\n",
    "        \n",
    "    l=[]\n",
    "    l.append(lambda1)\n",
    "    l.append(lambda2)\n",
    "    l.append(lambda3)\n",
    "    l.append(1-lambda1-lambda2-lambda3)\n",
    "    return (l)\n",
    "\n",
    "\n",
    "def interpol_score(file_name,bi_probab,tri_probab,probab,vocab):\n",
    "    score = 0\n",
    "    l = list(vocab.keys())\n",
    "    #opening the file, here held out corpus, with which we want to tune our interpolation parameters\n",
    "    with open(file_name,buffering=20000,encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            text = remove_punc(line)\n",
    "            tokens = text.split()\n",
    "            data = [tuple(tokens[i:i+4]) for i in range(len(tokens)-4+1)]            \n",
    "            for quad in data:\n",
    "                counts = []\n",
    "                flag = 0\n",
    "                for ele in quad:\n",
    "                    if ele in vocab:\n",
    "                        counts.append(str(vocab[ele]))\n",
    "                    else:\n",
    "                        flag = 1\n",
    "                        break\n",
    "                if flag == 0:\n",
    "                    tri = ' '.join(counts).split()\n",
    "                    w = tri[-1]\n",
    "                    del tri[-1]\n",
    "                    s = ' '.join(tri)\n",
    "                    #if predicted word is same as the word in corpus, we add 1 to our score\n",
    "                    predict = predictWord(s,bi_probab,tri_probab,probab,vocab)\n",
    "                    if w == predict:\n",
    "                        score = score + 1\n",
    "    #file closed                    \n",
    "    f.close()\n",
    "    return score\n",
    "\n",
    "def predictWord(s,bi_probab,tri_probab,probab,vocab):\n",
    "    predict = \"\"\n",
    "    l = s.split()\n",
    "    tri = ' '.join(s[1:3])\n",
    "    bi = s[2]\n",
    "    #backoff principle used for prediction here\n",
    "    #if string exists in quadgram probability dictionary\n",
    "    if s in probab:\n",
    "        predict = list(probab[s].keys())[-1]\n",
    "    #backing off to trigram probability dictionary\n",
    "    elif tri in tri_probab:\n",
    "        predict = list(tri_probab[tri].keys())[-1]\n",
    "    #backing off to bigram probability dictionary\n",
    "    elif bi in bi_probab:\n",
    "        predict = list(bi_probab[bi].keys())[-1]\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the weights using Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weights(l):\n",
    "    \n",
    "    #finding lambda1,lambda2 and lambda3 as any random value between 0.01-0.33\n",
    "    i=float(\"%0.2f\" %(random.uniform(0.01,0.33)))\n",
    "    j=float(\"%0.2f\" %(random.uniform(0.01,0.33)))\n",
    "    k=float(\"%0.2f\" %(random.uniform(0.01,0.33)))\n",
    "    m=1-(i+j+k)\n",
    "    l.append(i)\n",
    "    l.append(j)\n",
    "    l.append(k)\n",
    "    l.append(m)\n",
    "    \n",
    "                            \n",
    "    return l\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating score of the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scoreCalc(quad,tri,tokenList2):\n",
    "    score=0\n",
    "    scorepred=OrderedDict()\n",
    "    scorepred=OrderedDict.fromkeys(tokenList2,0)\n",
    "\n",
    "    #if predicted word is present in the vocabulary, we add 1 to our score\n",
    "    for item in quad:\n",
    "        if item[0:3] in tri:\n",
    "            scorepred[item[3]]+=1\n",
    "        v=list(scorepred.values())\n",
    "        k=list(scorepred.keys())\n",
    "        if (k[v.index(max(v))]==item[3]):\n",
    "            score+=1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-K smoothing\n",
    "Simply adding k to the numerator (count of words) and kV to the denominator (count of words) while computing probability.\n",
    "THe value of k is taken as user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trismoothk(tokens,tri,trigramSet2,smooth_tri,k):\n",
    "    \n",
    "    #Add k Smoothing for trigram model\n",
    "    \n",
    "    i=len(trigramSet2)\n",
    "    for item in trigramSet2:\n",
    "        smooth_tri[item]=tri.count(item) + k/float((tokens.count(item[0:2])+ i))\n",
    "        \n",
    "def quadsmoothk(tokens,quad,quadgramSet2,smooth_quad,k):\n",
    "    \n",
    "    #Add k Smoothing for quadgram model\n",
    "    \n",
    "    i=len(quadgramSet2)\n",
    "    for item in quadgramSet2:\n",
    "        smooth_quad[item]=quad.count(item) + k/float((tokens.count(item[0:3])+ i))\n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Smoothing\n",
    "Here we use the count of words we have seen once to predict the words we have not even seen once. Thus for a word which has occured 'c' times, we use the count of words which has occured 'c+1' times, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def goodturing(quadgramSet2,quadgramcount,wcount):\n",
    "    \n",
    "    c=0\n",
    "    c_star=0\n",
    "    n=len(quadgramSet2)\n",
    "    \n",
    "    wprob=defaultdict(dict)\n",
    "    \n",
    "    freq = {}\n",
    "    #storing the frequencies of count of ngrams\n",
    "    for item in wcount:\n",
    "        freq[wcount[item]]=0\n",
    "    for item in wcount:\n",
    "        freq[wcount[item]]=freq[wcount[item]]+1\n",
    "    #storing frequencies of count of ngrams in the test set        \n",
    "    for quad in quadgramcount:\n",
    "        if quadgramcount[quad] not in freq:\n",
    "            val = 0\n",
    "            for four in quadgramcount:\n",
    "                if quadgramcount[quad] == quadgramcount[four]:\n",
    "                    val = val +1\n",
    "            freq[quadgramcount[quad]] = val\n",
    "            \n",
    "    for item in wcount:\n",
    "        if (wcount[item]==0):\n",
    "            wcount[item]=float(freq[1]/n)\n",
    "    for item in freq:\n",
    "        if(freq[item]==0):\n",
    "            freq[item]=1\n",
    "    \n",
    "    #computing revised count for each word given its trigram context\n",
    "    for item in quadgramSet2:\n",
    "        c=wcount[item[3]]\n",
    "        if c not in freq.keys():\n",
    "            freq[c]=1\n",
    "        if (c+1) not in freq.keys():\n",
    "            freq[c+1]=freq[c]+1\n",
    "            #print (freq[c])\n",
    "            #print (freq[c+1])\n",
    "        c_star=(c+1)*freq[c+1]/freq[c]\n",
    "        tri=item[0:3]\n",
    "        prob=c_star/n\n",
    "        wprob[tri][item[3]]=prob\n",
    "    \n",
    "    #sorting the smoothed dictionary    \n",
    "    for tri in wprob:\n",
    "        wprob[tri]=sorted(wprob[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "            \n",
    "    return wprob\n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser Ney Smoothing\n",
    "Here we put two ideas into use -- one, we subtract a specific given discount from our n-gram counts and two, we use the continuation probability of the word for the given context.\n",
    "Weight for the continuation probability is calculated from the discount and continuation counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kney(trigramCount,quadgramCount):\n",
    "    \n",
    "    kneser=defaultdict(dict)\n",
    "    d=0.75\n",
    "    lambda1=0\n",
    "    tri_context_count=''\n",
    "    word_count=''\n",
    "    \n",
    "    for item,val in quadgramCount.items():\n",
    "        if trigramCount[item[0:3]]==0 :\n",
    "            trigramCount[item[0:3]]=1\n",
    "        if quadgramCount[item]==0 :\n",
    "            quadgramCount[item]=1\n",
    "        #calculation of discounted probability\n",
    "        p_disc=float((quadgramCount[item]-d)/trigramCount[item[0:3]])\n",
    "        #for tri,val2 in od3.items():\n",
    "         #   if (tri[2]==item[3]):\n",
    "        tri_context_count=trigramCount[item[0:3]]\n",
    "        word_count=quadgramCount[item]\n",
    "        lambda1=float(d/tri_context_count)\n",
    "        #calculation of continuation probability\n",
    "        p_cont=float(lambda1*tri_context_count/word_count)\n",
    "        prob=p_disc+p_cont\n",
    "        #print (item[0])\n",
    "        kneser[item[0:3]][item[3]]=prob\n",
    "     \n",
    "    #sorting the smoothed dictionary\n",
    "    for tri in kneser:\n",
    "        kneser[tri]=sorted(kneser[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return kneser\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "A measure to judge how effective the language model is after performing smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadperp(tokenList2,smooth_quad,smooth_tri,quadgramSet2):\n",
    "    \n",
    "    #computing quadgram perplexity\n",
    "    n= len (tokenList2)\n",
    "    perplexity4=1.0\n",
    "    \n",
    "    for item in quadgramSet2:\n",
    "        perplexity4=perplexity4*(((1/float(smooth_quad[item]))*smooth_tri[item[0:3]])**(1./n))\n",
    "        \n",
    "    print (\"Quadgram Perplexity = %f\" %(perplexity4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Off\n",
    "Trigrams from testing corpus have been taken to predict the most probable word following each trigram using backoff. Probability with quadgram, trigram or bigram is displayed correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backoff(tokens,test_quadgrams,train_quadgrams,train_trigrams,train_bigrams,train_unigrams):\n",
    "    \n",
    "    for item in test_quadgrams:\n",
    "        print (item)\n",
    "        if(train_quadgrams[item]>0):\n",
    "            print (\"probability with quadgram %f\"\n",
    "                  %((train_quadgrams[item])/float(train_trigrams[item[0:3]])))\n",
    "        \n",
    "        elif(train_trigrams[item[0:3]]>0):\n",
    "            print (\"probability with trigram %f\"\n",
    "                  %((train_trigrams[item[1:4]])/float(train_bigrams[item[1:3]])))\n",
    "            \n",
    "        elif(train_bigrams[item[1:3]]>0):\n",
    "            print (\"probability with bigram %f\"\n",
    "                  %((train_bigrams[item[2:4]])/float(train_unigrams[item[2]]+1)))\n",
    "        elif(train_bigrams[item[2:4]]<=0):\n",
    "            print (\"Probability with unigram %f\" %((train_unigrams[item[3]])/float(len(tokens))))\n",
    "                    \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function calling all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #opening the training corpus\n",
    "    f=open('Data/LanguageModels/training_corpus.txt','r',encoding='latin1')\n",
    "    content=f.read()\n",
    "    token=content.split()\n",
    "\n",
    "    #splitting into tokens\n",
    "    tokenList=list(token)\n",
    "    tokenSet=set(tokenList)\n",
    "    vocab = {}\n",
    "    probab = {}\n",
    "    tri_probab = {}\n",
    "    bi_probab = {}\n",
    "    unigram = {}\n",
    "    bigram = {}\n",
    "    trigram = {}\n",
    "    quadgram = {}\n",
    "    time1=time.time()\n",
    "    n = train_data(unigram,bigram,trigram,quadgram,vocab)\n",
    "    bi_probability(unigram,bigram,bi_probab)\n",
    "    tri_probability(bigram,trigram,tri_probab)\n",
    "    quad_probability(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,probab,n)\n",
    "    print (\"Time taken for training the model and parameter tuning: \", time.time()-time1)\n",
    "\n",
    "    #listing the tokens into n-grams\n",
    "    unigrams=list(ngrams(token,1))\n",
    "    bigrams=list(ngrams(token,2))\n",
    "    trigrams=list(ngrams(token,3))\n",
    "    quadgrams=list(ngrams(token,4))\n",
    "    \n",
    "    #Preprocessing the training set\n",
    "    preprocess(tokenList)\n",
    "    od=Counter()\n",
    "    od=unigramize(unigrams)\n",
    "    od2=Counter()\n",
    "    od2=bigramize(bigrams)\n",
    "    od3=Counter()\n",
    "    od3=trigramize(trigrams)\n",
    "    od4=Counter()\n",
    "    od4=quadgramize(quadgrams)\n",
    "    \n",
    "    #Taking our input string\n",
    "    sent=input(\"Enter your test string: \")\n",
    "    list2=sent.split()\n",
    "    sent_tri=list(ngrams(list2,3))\n",
    "    x=len(sent_tri)\n",
    "    test_tri=sent_tri[x-1]\n",
    "    \n",
    "    #Normal prediction of the most probable word\n",
    "    time1=time.time()\n",
    "    quad_prob_table=Counter(od4)\n",
    "    quad_prob_table=quad_prob(od3,od4)\n",
    "    quad_pred_table=defaultdict(dict)\n",
    "    quad_pred_table=pred_table(quad_prob_table)\n",
    "    word=quad_pred_table[test_tri]\n",
    "    word=sorted(word.items(), key=lambda x: x[1], reverse=True)\n",
    "    print (\"The next word could be: \")    \n",
    "    print (word[0][0])\n",
    "    print (\"Time taken for prediction: \", time.time()-time1)\n",
    "    \n",
    "    #Prediction of the word after interpolation using weights\n",
    "    time1=time.time()\n",
    "    quad_pole_table=defaultdict(dict)\n",
    "    quad_pole_table=interpolation_table(od,od2,od3,od4)\n",
    "    word=quad_pole_table[test_tri]\n",
    "    word=sorted(word, key=lambda x: x[1], reverse=True)\n",
    "    print (\"After interpolation, the most probable word could be: \")\n",
    "    print (word[0][0])\n",
    "    print (\"Time taken for prediction: \", time.time()-time1)\n",
    "    \n",
    "    \n",
    "    with open('Data/LanguageModels/testing_corpus.txt','r',encoding='latin1') as f:\n",
    "            contents=f.read()\n",
    "            tokens=contents.split()\n",
    "            tokenList2=list(tokens)\n",
    "            \n",
    "    #creating the held-out set for interpolation\n",
    "    partitionFile('Data/LanguageModels/testing_corpus.txt',len(tokens))\n",
    "    \n",
    "    \n",
    "    with open('Data/LanguageModels/heldset_corpus.txt','r',encoding='latin1') as fheld:\n",
    "            hcontents=fheld.read()\n",
    "            htokens=contents.split()\n",
    "            htokenList=list(htokens)\n",
    "     \n",
    "    #Preprocessing the test set and held-out set\n",
    "    preprocess(tokens)\n",
    "    preprocess(htokens)\n",
    "    bi=list(ngrams(tokenList2,2))\n",
    "    tri=list(ngrams(tokenList2,3))\n",
    "    quad=list(ngrams(tokenList2,4))\n",
    "    hbi=list(ngrams(htokenList,2))\n",
    "    htri=list(ngrams(htokenList,3))\n",
    "    hquad=list(ngrams(htokenList,4))\n",
    "    bigramSet2=set(bi)\n",
    "    trigramSet2=set(tri)\n",
    "    quadgramSet2=set(quad)\n",
    "    hod=Counter()\n",
    "    hod=unigramize(htokens)\n",
    "    hod2=Counter()\n",
    "    hod2=bigramize(hbi)\n",
    "    hod3=Counter()\n",
    "    hod3=trigramize(htri)\n",
    "    hod4=Counter()\n",
    "    hod4=quadgramize(hquad)\n",
    "   \n",
    "    \n",
    "    #Prediction of the word after interpolation using customised weights\n",
    "    #time1=time.time()\n",
    "    #quad_cpole_table=defaultdict(dict)\n",
    "    #l=[]\n",
    "    #quad_cpole_table=cus_interpolation_table(hod,hod2,hod3,hod4,l)\n",
    "    #word=quad_cpole_table[test_tri]\n",
    "    #word=sorted(word, key=lambda x: x[1], reverse=True)\n",
    "    #print (\"After interpolation, the most probable word could be: \")\n",
    "    #print (quad_cpole_table.items())\n",
    "    #print (word[0][0])\n",
    "    #print (l)\n",
    "    #print (\"Time taken for prediction: \", time.time()-time1)\n",
    "    \n",
    "    \n",
    "    #Add K Smoothing\n",
    "    smooth_tri=OrderedDict()\n",
    "    smooth_tri=OrderedDict.fromkeys(trigramSet2,0)\n",
    "    smooth_quad=OrderedDict()\n",
    "    smooth_quad=OrderedDict.fromkeys(quadgramSet2,0)\n",
    "    k=int(input(\"We will perform Add k smoothing now. Enter value of k: \"))\n",
    "    time1=time.time()\n",
    "    trismoothk(tokens,tri,trigramSet2,smooth_tri,k)\n",
    "    quadsmoothk(tokens,quad,quadgramSet2,smooth_quad,k)\n",
    "    print (\"Time taken for Add k Smoothing: \", time.time()-time1)\n",
    "    \n",
    "    #Good Turing Smoothing\n",
    "    od=unigramize(tokens) \n",
    "    wprob=defaultdict(dict)\n",
    "    time1=time.time()\n",
    "    wprob=goodturing(quadgramSet2,od4,od)\n",
    "    word=wprob[test_tri]\n",
    "    word=sorted(word, key=lambda x: x[1], reverse=True)\n",
    "    print (\"After Good Turing Smoothing, the most probable word could be: \")\n",
    "    print (word[0][0])\n",
    "    print (\"Time taken for Good Turing Smoothing: \", time.time()-time1)\n",
    "    \n",
    "    #Kneser Ney Smoothing\n",
    "    wprob=defaultdict(dict)\n",
    "    time1=time.time()\n",
    "    wprob=kney(od3,od4)\n",
    "    word=wprob[test_tri]\n",
    "    word=sorted(word, key=lambda x: x[1], reverse=True)\n",
    "    print (\"After Kneser Ney Smoothing, the most probable word could be: \")\n",
    "    print (word[0][0])\n",
    "    print (\"Time taken for Kneser Ney Smoothing: \", time.time()-time1)\n",
    "    \n",
    "    #Calculating Score of the language model\n",
    "    time1=time.time()\n",
    "    print (\"Score of the language model is: \")\n",
    "    print (scoreCalc(quad,tri,tokenList2))\n",
    "    print (\"Time taken for calculating Score: \", time.time()-time1)\n",
    "    \n",
    "    #Perplexity\n",
    "    time1=time.time()\n",
    "    quadperp(tokenList2,smooth_quad,smooth_tri,quadgramSet2)\n",
    "    print (\"Time taken to compute perplexity: \", time.time()-time1)\n",
    "    \n",
    "    #Backoff\n",
    "    time1=time.time()\n",
    "    #backoff(tokens,quad,od4,od3,od2,od)\n",
    "    #print (\"Time taken to compute Backoff: \", time.time()-time1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training the model and parameter tuning:  189.94470596313477\n",
      "Enter your test string: I could not\n",
      "The next word could be: \n",
      "have\n",
      "Time taken for prediction:  0.40525007247924805\n",
      "After interpolation, the most probable word could be: \n",
      "be\n",
      "Time taken for prediction:  1.124948263168335\n",
      "We will perform Add k smoothing now. Enter value of k: 4\n",
      "Time taken for Add k Smoothing:  33.70561981201172\n",
      "After Good Turing Smoothing, the most probable word could be: \n",
      "hope\n",
      "Time taken for Good Turing Smoothing:  0.11780452728271484\n",
      "After Kneser Ney Smoothing, the most probable word could be: \n",
      "walk\n",
      "Time taken for Kneser Ney Smoothing:  1.0115365982055664\n",
      "Score of the language model is: \n",
      "549\n",
      "Time taken for calculating Score:  11.336305141448975\n",
      "Quadgram Perplexity = 1.052211\n",
      "Time taken to compute perplexity:  0.018884897232055664\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
