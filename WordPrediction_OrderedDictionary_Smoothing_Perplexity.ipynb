{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction \n",
    "(Using an OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "f=open('Data/LanguageModels/training_corpus.txt','r',encoding='latin1')\n",
    "content=f.read()\n",
    "#token=nltk.word_tokenize(nltk.corpus.gutenberg.raw(nltk.corpus.gutenberg.fileids()[0]))\n",
    "#content=\"I like to play likKe . ;to sleep? football in the rain with my shoes on my shoes on and dancing in the rain with blah\"\n",
    "token=content.split()\n",
    "\n",
    "#splitting into tokens\n",
    "tokenList=list(token)\n",
    "tokenSet=set(tokenList)\n",
    "#print (\"Length of token list: \"+ str(len(token)))\n",
    "\n",
    "#storing stop words separately\n",
    "stop = set(stopwords.words('english'))\n",
    "#print (len(stop))\n",
    "#tokenSet=tokenSet-stop\n",
    "\n",
    "\n",
    "#listing the tokens into n-grams\n",
    "unigrams=list(ngrams(token,1))\n",
    "bigrams=list(ngrams(token,2))\n",
    "trigrams=list(ngrams(token,3))\n",
    "quadgrams=list(ngrams(token,4))\n",
    "pentgrams=list(ngrams(token,5))\n",
    "bigramSet=set(bigrams)\n",
    "trigramSet=set(trigrams)\n",
    "quadgramSet=set(quadgrams)\n",
    "pentgramSet=set(pentgrams)\n",
    "\n",
    "#creating set of unique words\n",
    "\n",
    "print (\"Number of unique words: \"+str(len(tokenSet)))\n",
    "#print (\"Total number of words: \"+ str(len(tokenList)))\n",
    "#print (\"Number of trigrams: \"+str(len(trigrams)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the training set\n",
    "\n",
    "Preprocessing the input training corpus by converting into lowercase, removing punctuations and removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing the input text\n",
    "i=0\n",
    "for word1 in tokenList:\n",
    "    #conversion into lowercase\n",
    "    word1=word1.lower()\n",
    "    #Takes Care of Multiple Punctuation Marks\n",
    "    word1=word1.replace('.','').replace(',','').replace(':','').replace(';','').replace('!','').replace('?','').replace('(','').replace(')','').replace('-','').replace('_','').replace('\\\\',' ').replace('\\\"',' ').replace('\\'',' ')      \n",
    "    #translator=word1.maketrans('','',string.punctuation)\n",
    "    #word1=word1.translate(translator)\n",
    "    #tokenList[i]=word1\n",
    "    #for l in string.punctuation:\n",
    "     #   if l in word1:\n",
    "    #      word1=word1.replace(l,' ')\n",
    "    tokenList[i]=word1        \n",
    "    i=i+1\n",
    "#print (tokenList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary to store trigrams from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "od3=Counter()\n",
    "\n",
    "\n",
    "for item in trigrams:\n",
    "    od3[item]+=1\n",
    "            \n",
    "#print (od3.keys())\n",
    "#print (od3.values())\n",
    "#print (len(od3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary to store quadgrams from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "od4=Counter()\n",
    "\n",
    "for item in quadgrams:\n",
    "    od4[item]+=1\n",
    "            \n",
    "#print (od4.keys())\n",
    "#print (od4.values())\n",
    "#print (len(od4))\n",
    "#print (len(trigramSet))\n",
    "#print (len(quadgramSet))\n",
    "tokenSet=set(tokenList)\n",
    "#print (len(tokenSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking our input string, which we will use for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your test string: Emma could have\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Taking our input string\n",
    "sent=input(\"Enter your test string: \")\n",
    "list2=sent.split()\n",
    "test=' '.join(list2[(len(list2)-3):len(list2)])\n",
    "#test=input(\"Enter your test string: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the probability table with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "max_freq=0\n",
    "freq=0\n",
    "\n",
    "\n",
    "prob=OrderedDict()\n",
    "prob=OrderedDict.fromkeys(quadgramSet,0)\n",
    "\n",
    "for item,value in od4.items():\n",
    "    for item2,value2 in od3.items():\n",
    "        #print (item)\n",
    "        if(item2 in item):\n",
    "            prob[item]=value2/value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Predicting the most likely word to occur:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print (prob.values())\n",
    "del(list2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prediction(od4,test):\n",
    "    \n",
    "    pred=OrderedDict()\n",
    "    pred=OrderedDict.fromkeys(token,0)\n",
    "\n",
    "    for item,value in od4.items():\n",
    "        out=\" \".join([str(s) for s in list(item)])\n",
    "        if not(out.find(test)):\n",
    "            pred[item[3]]+=(1)\n",
    "\n",
    "        \n",
    "    #OrderedDict(sorted(od4.items(), key=lambda t: t[1],reverse=True))        \n",
    "    v=list(pred.values())\n",
    "    k=list(pred.keys())\n",
    "    \n",
    "        \n",
    "    #print (pred.items())  \n",
    "    return (k[v.index(max(v))])\n",
    "    \n",
    "print (prediction(od4,test))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Computing the score of the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#computing score of the language model\n",
    "score=0\n",
    "with open('Data/LanguageModels/testing_corpus.txt','r',encoding='latin1') as f:\n",
    "        contents=f.read()\n",
    "        tokens=contents.split()\n",
    "        tokenList2=list(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the test file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing the input text\n",
    "i=0\n",
    "for word1 in tokens:\n",
    "    #conversion into lowercase\n",
    "    word1=word1.lower()\n",
    "    #Takes Care of Multiple Punctuation Marks\n",
    "    word1=word1.replace('.',' ').replace(',',' ').replace(':',' ').replace(';',' ').replace('!',' ').replace('?',' ').replace('(',' ').replace(')',' ').replace('-',' ').replace('_',' ').replace('\\\\',' ').replace('\\\"',' ').replace('\\'',' ')      \n",
    "    #translator=word1.maketrans('','',string.punctuation)\n",
    "    #word1=word1.translate(translator)\n",
    "    #tokenList[i]=word1\n",
    "    #for l in string.punctuation:\n",
    "     #   if l in word1:\n",
    "    #      word1=word1.replace(l,' ')\n",
    "    tokenList2[i]=word1        \n",
    "    i=i+1\n",
    "\n",
    "#print(tokenList2)\n",
    "bi=list(ngrams(tokenList2,2))\n",
    "tri=list(ngrams(tokenList2,3))\n",
    "quad=list(ngrams(tokenList2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551\n"
     ]
    }
   ],
   "source": [
    "def scoreCalc(quad,tri,tokenList2):\n",
    "    score=0\n",
    "    scorepred=OrderedDict()\n",
    "    scorepred=OrderedDict.fromkeys(tokenList2,0)\n",
    "\n",
    "    for item in quad:\n",
    "        if item[0:3] in tri:\n",
    "            scorepred[item[3]]+=1\n",
    "        v=list(scorepred.values())\n",
    "        k=list(scorepred.keys())\n",
    "        if (k[v.index(max(v))]==item[3]):\n",
    "            score+=1\n",
    "\n",
    "    return score\n",
    "    \n",
    "print (scoreCalc(quad,tri,tokenList2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#computing unigram perplexity\n",
    "bigramSet2=set(bi)\n",
    "trigramSet2=set(tri)\n",
    "quadgramSet2=set(quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add 1 Smoothing for bigram model\n",
    "smooth_bi=OrderedDict()\n",
    "smooth_bi=OrderedDict.fromkeys(bigramSet2,0)\n",
    "i=0\n",
    "    \n",
    "for item in bigramSet2:\n",
    "    #smooth_bi.append(bi.count(item) + 1/float((token.count(item[0])+ len(bigramSet2))))\n",
    "    smooth_bi[item]=bi.count(item) + 1/float((tokens.count(item[0])+ len(bigramSet2)))\n",
    "    #print (\"%s ->  %f\" %(item,counting[i]/float((tokens.count(item[0])+ len(bigramSet2)))))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add 1 Smoothing for trigram model\n",
    "smooth_tri=OrderedDict()\n",
    "smooth_tri=OrderedDict.fromkeys(trigramSet2,0)\n",
    "i=0\n",
    "    \n",
    "for item in trigramSet2:\n",
    "    #smooth_bi.append(bi.count(item) + 1/float((token.count(item[0])+ len(bigramSet2))))\n",
    "    smooth_tri[item]=tri.count(item) + 1/float((tokens.count(item[0:2])+ len(trigramSet2)))\n",
    "    #print (\"%s ->  %f\" %(item,counting[i]/float((tokens.count(item[0])+ len(bigramSet2)))))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add 1 Smoothing for quadgram model\n",
    "smooth_quad=OrderedDict()\n",
    "smooth_quad=OrderedDict.fromkeys(quadgramSet2,0)\n",
    "i=0\n",
    "    \n",
    "for item in quadgramSet2:\n",
    "    #smooth_bi.append(bi.count(item) + 1/float((token.count(item[0])+ len(bigramSet2))))\n",
    "    smooth_quad[item]=quad.count(item) + 1/float((tokens.count(item[0:3])+ len(quadgramSet2)))\n",
    "    #print (\"%s ->  %f\" %(item,counting[i]/float((tokens.count(item[0])+ len(bigramSet2)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram perplexity = 610.804324\n",
      "Bigram perplexity = 6.600619\n"
     ]
    }
   ],
   "source": [
    "#computing unigram perplexity\n",
    "perplexity1=float(1.0)\n",
    "n= len (tokenList2)\n",
    "i=0\n",
    "for item in tokenList2:\n",
    "    perplexity1=perplexity1*((1/float(tokenList2.count(item)))**(1./n))\n",
    "perplexity1=(perplexity1)*len(tokenList2)\n",
    "print (\"Unigram perplexity = %f\" %(perplexity1))\n",
    "\n",
    "#computing bigram perplexity\n",
    "\n",
    "perplexity2=1.0\n",
    "i=0\n",
    "k=0\n",
    "n= len (tokens)\n",
    "for item in bigramSet2:\n",
    "    #perplexity2= perplexity2*(((1/float(bi.count(item)))*tokenList2.count(item[0]))**(1./n))\n",
    "    perplexity2= perplexity2*(((1/float(smooth_bi[item]))*tokenList2.count(item[0]))**(1./n))\n",
    "    #print (\"%f\" %(tokenList2.count(item[0])))\n",
    "print (\"Bigram perplexity = %f\" %(perplexity2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram perplexity = 1.651014\n"
     ]
    }
   ],
   "source": [
    "#computing trigram perplexity\n",
    "\n",
    "perplexity3=1.0\n",
    "i=0\n",
    "for item in trigramSet2:\n",
    "       perplexity3=perplexity3*(((1/float(smooth_tri[item]))*smooth_bi[item[0:2]])**(1./n))\n",
    "       i=i+1\n",
    "#perplexity3 = perplexity3)) * ((n+k)/n)\n",
    "print (\"Trigram perplexity = %f\" %(perplexity3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadgram Perplexity = 1.060338\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#computing quadgram perplexity\n",
    "\n",
    "perplexity4=1.0\n",
    "i=0\n",
    "for item in quadgramSet2:\n",
    "    perplexity4=perplexity4*(((1/float(smooth_quad[item]))*smooth_tri[item[0:3]])**(1./n))\n",
    "    i=i+1\n",
    "print (\"Quadgram Perplexity = %f\" %(perplexity4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
