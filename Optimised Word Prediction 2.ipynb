{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import time\n",
    "import random\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tokenList):\n",
    "    i=0\n",
    "    for word1 in tokenList:\n",
    "    #conversion into lowercase\n",
    "        word1=word1.lower()\n",
    "    #Takes Care of Multiple Punctuation Marks\n",
    "        word1=word1.replace('.','').replace(',','').replace(':','').replace(';','').replace('!','').replace('?','').replace('(','').replace(')','').replace('-','').replace('_','').replace('\\\\',' ').replace('\\\"',' ').replace('\\'',' ')      \n",
    "    \n",
    "        tokenList[i]=word1        \n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating unigram,bigram,trigram,quadgram dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigramize(unigrams):\n",
    "    od=Counter()\n",
    "    for item in unigrams:\n",
    "        od[item]+=1\n",
    "    return od\n",
    "\n",
    "def bigramize(bigrams):\n",
    "    od2=Counter()\n",
    "    for item in bigrams:\n",
    "        od2[item]+=1\n",
    "    return od2\n",
    "\n",
    "def trigramize(trigrams):\n",
    "    od3=Counter()\n",
    "    for item in trigrams:\n",
    "        od3[item]+=1\n",
    "    return od3\n",
    "\n",
    "def quadgramize(quadgrams):\n",
    "    od4=Counter()\n",
    "    for item in quadgrams:\n",
    "        od4[item]+=1\n",
    "    return od4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def i_unigrams(unigrams,unigram):\n",
    "    for uni in unigrams:\n",
    "        stri = ''.join(uni)\n",
    "        if not unigram or stri not in unigram:\n",
    "            unigram[stri] = 1;\n",
    "        else:\n",
    "            unigram[stri] = unigram[stri] + 1\n",
    "            \n",
    "def i_bigrams(bigrams,bigram):\n",
    "    for bi in bigrams:\n",
    "        stri = ' '.join(bi)\n",
    "        if not bigram or stri not in bigram:\n",
    "            bigram[stri] = 1;\n",
    "        else:\n",
    "            bigram[stri] = bigram[stri] + 1\n",
    "            \n",
    "\n",
    "def i_trigrams(trigrams,trigram):\n",
    "    for tri in trigrams:\n",
    "        stri = ' '.join(tri)\n",
    "        if not trigram or stri not in trigram:\n",
    "            trigram[stri] = 1;\n",
    "        else:\n",
    "            trigram[stri] = trigram[stri] + 1\n",
    "            \n",
    "def i_quadgrams(quadgrams,quadgram):\n",
    "    for quad in quadgrams:\n",
    "        stri = ' '.join(quad)\n",
    "        if not quadgram or stri not in quadgram:\n",
    "            quadgram[stri] = 1;\n",
    "        else:\n",
    "            quadgram[stri] = quadgram[stri] + 1\n",
    "            \n",
    "def create_token_list(tokens,vocab):\n",
    "    newlist=[]\n",
    "    for word in tokens:\n",
    "        if word:\n",
    "            if not vocab or word not in vocab: \n",
    "                vocab[word] = len(vocab)\n",
    "            newlist.append(str(vocab[word]))\n",
    "    return newlist\n",
    "\n",
    "def remove_punc(text):\n",
    "    for c in string.punctuation:\n",
    "        if c!='\\'':\n",
    "            text=text.replace(c,\" \")\n",
    "    text = text.replace(\"' \",\" \")\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    answer = text.lower()\n",
    "    return answer\n",
    "\n",
    "def tokenize(text, ngrams,vocab):\n",
    "    clean_text = remove_punc(text)\n",
    "    tokens = create_token_list(clean_text.split(),vocab)\n",
    "    #print(tokens)\n",
    "    return [tuple(tokens[i:i+ngrams]) for i in range(len(tokens)-ngrams+1)]\n",
    "\n",
    "def train_data(unigram,bigram,trigram,quadgram,vocab):\n",
    "    l = 0\n",
    "    with open('training_corpus.txt',buffering=20000,encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            data = tokenize(line,1,vocab)\n",
    "            i_unigrams(data,unigram)\n",
    "            l = l + len(data)\n",
    "            data = tokenize(line,2,vocab)\n",
    "            i_bigrams(data,bigram)\n",
    "            data = tokenize(line,3,vocab)\n",
    "            i_trigrams(data,trigram)\n",
    "            data = tokenize(line,4,vocab)\n",
    "            i_quadgrams(data,quadgram)\n",
    "    f.close()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating probability dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bi_probability(unigram,bigram,bi_probab):\n",
    "    for bi in bigram:\n",
    "        s = bi.split()\n",
    "        w = s[1]\n",
    "        stri = s[0]\n",
    "        p = float(bigram[bi])/unigram[stri]\n",
    "        if stri in bi_probab:\n",
    "            if w not in bi_probab[stri]:\n",
    "                bi_probab[stri][w] = p\n",
    "                d = OrderedDict(sorted(bi_probab[stri].items(), key=lambda t: t[1]))\n",
    "                bi_probab[stri] = d\n",
    "        else:\n",
    "            bi_probab[stri] = {}\n",
    "            bi_probab[stri][w] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tri_probability(bigram,trigram,tri_probab):\n",
    "    for tri in trigram:\n",
    "        s = tri.split()\n",
    "        w = s[-1]\n",
    "        stri = ' '.join(s[0:2])\n",
    "        p = float(trigram[tri])/bigram[stri]\n",
    "        if stri in tri_probab:\n",
    "            if w not in tri_probab[stri]:\n",
    "                tri_probab[stri][w] = p\n",
    "                d = OrderedDict(sorted(tri_probab[stri].items(), key=lambda t: t[1]))\n",
    "                tri_probab[stri] = d\n",
    "        else:\n",
    "            tri_probab[stri] = {}\n",
    "            tri_probab[stri][w] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quad_probability(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,probab,tokens):\n",
    "    l = grid_search(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,tokens)\n",
    "    #lambda1 = 0.25\n",
    "    #lambda2 = 0.25\n",
    "    #lambda3 = 0.25\n",
    "    #lambda4 = 0.25\n",
    "    lambda1 = l[0]\n",
    "    lambda2 = l[1]\n",
    "    lambda3 = l[2]\n",
    "    lambda4 = l[3]\n",
    "    for quad in quadgram:\n",
    "        s = quad.split()\n",
    "        w = s[-1]\n",
    "        stri = ' '.join(s[0:3])\n",
    "        p = interpolation(quad,stri,s,quadgram,trigram,bigram,unigram,tokens,lambda1,lambda2,lambda3,lambda4)\n",
    "        if stri in probab:\n",
    "            if w not in probab[stri]:\n",
    "                probab[stri][w] = p\n",
    "                d = OrderedDict(sorted(probab[stri].items(), key=lambda t: t[1]))\n",
    "                probab[stri] = d\n",
    "        else:\n",
    "            probab[stri] = {}\n",
    "            probab[stri][w] = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating quadgram probability table and predicting the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quad_prob(trigrams,quadgrams):\n",
    "    \n",
    "    tri_freq=Counter(trigrams)\n",
    "    quad_freq=Counter(quadgrams)\n",
    "    #print (tri_freq.items())\n",
    "    \n",
    "    for item in quad_freq:\n",
    "        tri=item[0:3]\n",
    "        quad_freq[item]=(quad_freq[item]/tri_freq[tri])\n",
    "        \n",
    "    return quad_freq\n",
    "\n",
    "def pred_table(quad_prob_table):\n",
    "    quad_pred_table=defaultdict(dict)\n",
    "    \n",
    "    for quad in quad_prob_table:\n",
    "        prob=quad_prob_table[quad]\n",
    "        tri=quad[0:3]\n",
    "        token=quad[3]\n",
    "        quad_pred_table[tri][token]=prob\n",
    "        \n",
    "    for tri in quad_pred_table:\n",
    "        quad_pred_table[tri]=sorted(quad_pred_table[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return quad_pred_table\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Interpolation using given weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolation_table(od,od2,od3,od4):\n",
    "    \n",
    "    pole=defaultdict(dict)\n",
    "    \n",
    "    i=0\n",
    "    s=0.0\n",
    "    lambda1=0.25\n",
    "    lambda2=0.25\n",
    "    lambda3=0.25\n",
    "    lambda4=0.25\n",
    "    \n",
    "    for item,value in od4.items():\n",
    "        #if i>100:\n",
    "         #   break    \n",
    "        p=lambda1*float((value)/od3[item[0:3]]) + lambda2*float((od3[item[1:4]])/(od2[item[1:3]])) + lambda3*float((od2[item[2:4]])/(od[item[2]]+1)) +lambda4*(float((od[item[3]]+1)))\n",
    "        pole[item[0:3]][item[3]]=p    \n",
    "        \n",
    "    for tri in pole:\n",
    "        pole[tri]=sorted(pole[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation using customised weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cus_interpolation_table(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,tokens):\n",
    "\n",
    "    pole=defaultdict(dict)\n",
    "    pmax=0\n",
    "    i=0\n",
    "                         \n",
    "    for item,value in od4.items(): \n",
    "        \n",
    "        for i in range (10):\n",
    "            #l=weights(l)\n",
    "            l=grid_search(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,tokens)\n",
    "            p=l[0]*float((value)/od3[item[0:3]]) + l[1]*float((od3[item[1:4]])/(od2[item[1:3]])) + l[2]*float((od2[item[2:4]])/(od[item[2]]+1)) +l[3]*(float((od[item[3]]+1)))\n",
    "            if(p>pmax):\n",
    "                pole[item[0:3]][item[3]]=p\n",
    "                pmax=p\n",
    "                                \n",
    "    \n",
    "    for tri in pole:\n",
    "        pole[tri]=sorted(pole[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning testing corpus to create the held-out corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loads the testing corpus and partitions it into equal halves to create testing corpus and held-out corpus\n",
    "def partitionFile(file_path,word_count):\n",
    "    token = []\n",
    "    word_count = int(word_count*0.5)\n",
    "      \n",
    "    pos = 0\n",
    "    word_len = 0\n",
    "\n",
    "    #open the corpus file and read it line by line\n",
    "    file = open(file_path,'r')\n",
    "    test_file = open('testing_corpus.txt','w')\n",
    "    held_file = open('heldset_corpus.txt','w') \n",
    "\n",
    "    line = file.readline()\n",
    "\n",
    "    while line:\n",
    "        #split the line into tokens\n",
    "        token = line.split()\n",
    "\n",
    "        #write the line to the training file\n",
    "        test_file.write(line)\n",
    "\n",
    "        word_len = word_len + len(token)  \n",
    "            \n",
    "        #quit training when 90% of the corpus has been read\n",
    "        if word_len >= word_count:\n",
    "                pos = file.tell()\n",
    "                break;\n",
    "            \n",
    "        line = file.readline()\n",
    "\n",
    "    #Prepare the testing data\n",
    "    if word_count <= word_len:\n",
    "        file.seek(pos)\n",
    "        test_data = file.read();\n",
    "        held_file.write(test_data)\n",
    "            \n",
    "    file.close()\n",
    "    test_file.close()\n",
    "    held_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the weights using grid search\n",
    "\n",
    "We find the weights of interpolation using grid search. For each set of lambda values, we find the score using our held out corpus. The set of lambda values giving us the maximised score is the one which we use for our interpolation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolation(quad,stri,s,quadgram,trigram,bigram,unigram,tokens,lambda1,lambda2,lambda3,lambda4):\n",
    "    p = ( lambda1 * (quadgram[quad]/trigram[stri])) \n",
    "    + (lambda2 * (trigram[' '.join(s[1:4])]/bigram[' '.join(s[1:3])])) \n",
    "    + (lambda3 * (bigram[' '.join(s[2:4])]/unigram[str(s[2])])) \n",
    "    + (lambda4 * (unigram[str(s[3])]/tokens))\n",
    "    return p\n",
    "\n",
    "def grid_search(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,tokens):\n",
    "    lambda1 = 0\n",
    "    lambda2 = 0\n",
    "    lambda3 = 0\n",
    "    max_score = 0\n",
    "    i=0\n",
    "    while i<=1:\n",
    "        j=0\n",
    "        while j<=1:\n",
    "            k=0\n",
    "            while k<=1:\n",
    "                if (i+j+k)<=1:\n",
    "                    probab_dict = {}\n",
    "                    for quad in quadgram:\n",
    "                        s = quad.split()\n",
    "                        w = s[-1]\n",
    "                        stri = ' '.join(s[0:3])\n",
    "                        p = interpolation(quad,stri,s,quadgram,trigram,bigram,unigram,tokens,i,j,k,(1-i-j-k))\n",
    "                        if stri in probab_dict:\n",
    "                            if w not in probab_dict[stri] :\n",
    "                                probab_dict[stri][w] = p\n",
    "                                d = OrderedDict(sorted(probab_dict[stri].items(), key=lambda t: t[1]))\n",
    "                                probab_dict[stri] = d\n",
    "                        else:\n",
    "                            probab_dict[stri] = {}\n",
    "                            probab_dict[stri][w] = p\n",
    "                    score = interpol_score('held_out_corpus.txt',bi_probab,tri_probab,probab_dict,vocab)\n",
    "                    #print(i,j,k,score,max_score)\n",
    "                    if score > max_score:\n",
    "                        lambda1 = i\n",
    "                        lambda2 = j\n",
    "                        lambda3 = k\n",
    "                        max_score = score\n",
    "                k = k + 0.1\n",
    "            j = j + 0.1\n",
    "        i = i + 0.1\n",
    "        \n",
    "    l=[]\n",
    "    l.append(lambda1)\n",
    "    l.append(lambda2)\n",
    "    l.append(lambda3)\n",
    "    l.append(1-lambda1-lambda2-lambda3)\n",
    "    return (l)\n",
    "\n",
    "\n",
    "def interpol_score(file_name,bi_probab,tri_probab,probab,vocab):\n",
    "    score = 0\n",
    "    l = list(vocab.keys())\n",
    "    with open(file_name,buffering=20000,encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            text = remove_punc(line)\n",
    "            tokens = text.split()\n",
    "            data = [tuple(tokens[i:i+4]) for i in range(len(tokens)-4+1)]            \n",
    "            for quad in data:\n",
    "                numbers = []\n",
    "                flag = 0\n",
    "                for ele in quad:\n",
    "                    if ele in vocab:\n",
    "                        numbers.append(str(vocab[ele]))\n",
    "                    else:\n",
    "                        flag = 1\n",
    "                        break\n",
    "                if flag == 0:\n",
    "                    tri = ' '.join(numbers).split()\n",
    "                    w = tri[-1]\n",
    "                    del tri[-1]\n",
    "                    s = ' '.join(tri)\n",
    "                    predict = predictWord(s,bi_probab,tri_probab,probab,vocab)\n",
    "                    if w == predict:\n",
    "                        score = score + 1\n",
    "                        \n",
    "    f.close()\n",
    "    return score\n",
    "\n",
    "def predictWord(s,bi_probab,tri_probab,probab,vocab):\n",
    "    predict = \"\"\n",
    "    l = s.split()\n",
    "    tri = ' '.join(s[1:3])\n",
    "    bi = s[2]\n",
    "    if s in probab:\n",
    "        predict = list(probab[s].keys())[-1]\n",
    "    elif tri in tri_probab:\n",
    "        predict = list(tri_probab[tri].keys())[-1]\n",
    "    elif bi in bi_probab:\n",
    "        predict = list(bi_probab[bi].keys())[-1]\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the weights using Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weights(l):\n",
    "    \n",
    "    i=float(\"%0.2f\" %(random.uniform(0.01,0.33)))\n",
    "    j=float(\"%0.2f\" %(random.uniform(0.01,0.33)))\n",
    "    k=float(\"%0.2f\" %(random.uniform(0.01,0.33)))\n",
    "    m=1-(i+j+k)\n",
    "    l.append(i)\n",
    "    l.append(j)\n",
    "    l.append(k)\n",
    "    l.append(m)\n",
    "    \n",
    "                            \n",
    "    return l\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating score of the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scoreCalc(quad,tri,tokenList2):\n",
    "    score=0\n",
    "    scorepred=OrderedDict()\n",
    "    scorepred=OrderedDict.fromkeys(tokenList2,0)\n",
    "\n",
    "    for item in quad:\n",
    "        if item[0:3] in tri:\n",
    "            scorepred[item[3]]+=1\n",
    "        v=list(scorepred.values())\n",
    "        k=list(scorepred.keys())\n",
    "        if (k[v.index(max(v))]==item[3]):\n",
    "            score+=1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add k smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trismoothk(tokens,tri,trigramSet2,smooth_tri,k):\n",
    "    \n",
    "    #Add k Smoothing for trigram model\n",
    "    \n",
    "    i=len(trigramSet2)\n",
    "    \n",
    "    for item in trigramSet2:\n",
    "        smooth_tri[item]=tri.count(item) + k/float((tokens.count(item[0:2])+ i))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadsmoothk(tokens,quad,quadgramSet2,smooth_quad,k):\n",
    "    \n",
    "    #Add k Smoothing for quadgram model\n",
    "    \n",
    "    i=len(quadgramSet2)\n",
    "    \n",
    "    for item in quadgramSet2:\n",
    "        smooth_quad[item]=quad.count(item) + k/float((tokens.count(item[0:3])+ i))\n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def goodturing(quadgramSet2,quadgramcount,wcount):\n",
    "    \n",
    "    c=0\n",
    "    c_star=0\n",
    "    n=len(quadgramSet2)\n",
    "    \n",
    "    wprob=defaultdict(dict)\n",
    "    \n",
    "    freq = {}\n",
    "    \n",
    "    for item in wcount:\n",
    "        freq[wcount[item]]=0\n",
    "    for item in wcount:\n",
    "        freq[wcount[item]]=freq[wcount[item]]+1\n",
    "            \n",
    "    for quad in quadgramcount:\n",
    "        if quadgramcount[quad] not in freq:\n",
    "            val = 0\n",
    "            for four in quadgramcount:\n",
    "                if quadgramcount[quad] == quadgramcount[four]:\n",
    "                    val = val +1\n",
    "            freq[quadgramcount[quad]] = val\n",
    "            \n",
    "    for item in wcount:\n",
    "        if (wcount[item]==0):\n",
    "            wcount[item]=float(freq[1]/n)\n",
    "    for item in freq:\n",
    "        if(freq[item]==0):\n",
    "            freq[item]=1\n",
    "    \n",
    "    for item in quadgramSet2:\n",
    "        c=wcount[item[3]]\n",
    "        if c not in freq.keys():\n",
    "            freq[c]=1\n",
    "        if (c+1) not in freq.keys():\n",
    "            freq[c+1]=freq[c]+1\n",
    "            #print (freq[c])\n",
    "            #print (freq[c+1])\n",
    "        c_star=(c+1)*freq[c+1]/freq[c]\n",
    "        tri=item[0:3]\n",
    "        prob=c_star/n\n",
    "        wprob[tri][item[3]]=prob\n",
    "        \n",
    "    for tri in wprob:\n",
    "        wprob[tri]=sorted(wprob[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "            \n",
    "    return wprob\n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser Ney Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kney(od3,od4):\n",
    "    \n",
    "    kneser=defaultdict(dict)\n",
    "    d=0.25\n",
    "    lambda1=0.75\n",
    "    n1=0\n",
    "    n2=0\n",
    "    \n",
    "    for item,val1 in od4.items():\n",
    "        disc=float((od4[item]-d)/od3[item[0:3]])\n",
    "        #for tri,val2 in od3.items():\n",
    "         #   if (tri[2]==item[3]):\n",
    "        n1=od3[item[0:3]]\n",
    "        n2=od4[item]\n",
    "        cont=float(lambda1*n1/n2)\n",
    "        prob=disc+cont\n",
    "        kneser[item[0:3]][item[3]]=prob\n",
    "        \n",
    "    for tri in kneser:\n",
    "        kneser[tri]=sorted(kneser[tri].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return kneser\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadperp(tokenList2,smooth_quad,smooth_tri,quadgramSet2):\n",
    "    \n",
    "    #computing quadgram perplexity\n",
    "    n= len (tokenList2)\n",
    "    perplexity4=1.0\n",
    "    \n",
    "    for item in quadgramSet2:\n",
    "        perplexity4=perplexity4*(((1/float(smooth_quad[item]))*smooth_tri[item[0:3]])**(1./n))\n",
    "        \n",
    "    print (\"Quadgram Perplexity = %f\" %(perplexity4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backoff(tokens,test_quadgrams,train_quadgrams,train_trigrams,train_bigrams,train_unigrams):\n",
    "    \n",
    "    for item in test_quadgrams:\n",
    "        print (item)\n",
    "        if(train_quadgrams[item]>0):\n",
    "            print (\"probability with quadgram %f\"\n",
    "                  %((train_quadgrams[item])/float(train_trigrams[item[0:3]])))\n",
    "        \n",
    "        elif(train_trigrams[item[0:3]]>0):\n",
    "            print (\"probability with trigram %f\"\n",
    "                  %((train_trigrams[item[1:4]])/float(train_bigrams[item[1:3]])))\n",
    "            \n",
    "        elif(train_bigrams[item[1:3]]>0):\n",
    "            print (\"probability with bigram %f\"\n",
    "                  %((train_bigrams[item[2:4]])/float(train_unigrams[item[2]]+1)))\n",
    "        elif(train_bigrams[item[2:4]]<=0):\n",
    "            print (\"Probability with unigram %f\" %((train_unigrams[item[3]])/float(len(tokens))))\n",
    "                    \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function calling all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    f=open('Data/LanguageModels/training_corpus.txt','r',encoding='latin1')\n",
    "    content=f.read()\n",
    "    token=content.split()\n",
    "\n",
    "    #splitting into tokens\n",
    "    tokenList=list(token)\n",
    "    tokenSet=set(tokenList)\n",
    "    vocab = {}\n",
    "    probab = {}\n",
    "    tri_probab = {}\n",
    "    bi_probab = {}\n",
    "    unigram = {}\n",
    "    bigram = {}\n",
    "    trigram = {}\n",
    "    quadgram = {}\n",
    "    #n = train_data(unigram,bigram,trigram,quadgram,vocab)\n",
    "    #bi_probability(unigram,bigram,bi_probab)\n",
    "    #tri_probability(bigram,trigram,tri_probab)\n",
    "    #quad_probability(vocab,bi_probab,tri_probab,unigram,bigram,trigram,quadgram,probab,n)\n",
    "\n",
    "\n",
    "    #listing the tokens into n-grams\n",
    "    unigrams=list(ngrams(token,1))\n",
    "    bigrams=list(ngrams(token,2))\n",
    "    trigrams=list(ngrams(token,3))\n",
    "    quadgrams=list(ngrams(token,4))\n",
    "    \n",
    "    #Preprocessing the training set\n",
    "    preprocess(tokenList)\n",
    "    \n",
    "    od=Counter()\n",
    "    od=unigramize(unigrams)\n",
    "    \n",
    "    od2=Counter()\n",
    "    od2=bigramize(bigrams)\n",
    "    \n",
    "    od3=Counter()\n",
    "    od3=trigramize(trigrams)\n",
    "    \n",
    "    \n",
    "    od4=Counter()\n",
    "    od4=quadgramize(quadgrams)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Taking our input string\n",
    "    sent=input(\"Enter your test string: \")\n",
    "    list2=sent.split()\n",
    "    #test=' '.join(list2[(len(list2)-3):len(list2)])\n",
    "    sent_tri=list(ngrams(list2,3))\n",
    "    x=len(sent_tri)\n",
    "    test_tri=sent_tri[x-1]\n",
    "    \n",
    "\n",
    "    #Normal prediction of the most probable word\n",
    "    time1=time.time()\n",
    "    quad_prob_table=Counter(od4)\n",
    "    quad_prob_table=quad_prob(od3,od4)\n",
    "    quad_pred_table=defaultdict(dict)\n",
    "    quad_pred_table=pred_table(quad_prob_table)\n",
    "    word=quad_pred_table[test_tri]\n",
    "    word=sorted(word.items(), key=lambda x: x[1], reverse=True)\n",
    "    print (\"The next word could be: \")    \n",
    "    print (word[0][0])\n",
    "    print (\"Time taken for prediction: \", time.time()-time1)\n",
    "    \n",
    "    #Prediction of the word after interpolation using given weights\n",
    "    time1=time.time()\n",
    "    quad_pole_table=defaultdict(dict)\n",
    "    quad_pole_table=interpolation_table(od,od2,od3,od4)\n",
    "    word=quad_pole_table[test_tri]\n",
    "    word=sorted(word, key=lambda x: x[1], reverse=True)\n",
    "    print (\"After interpolation, the most probable word could be: \")\n",
    "    print (word[0][0])\n",
    "    print (\"Time taken for prediction: \", time.time()-time1)\n",
    "    \n",
    "    \n",
    "    with open('Data/LanguageModels/testing_corpus.txt','r',encoding='latin1') as f:\n",
    "            contents=f.read()\n",
    "            tokens=contents.split()\n",
    "            tokenList2=list(tokens)\n",
    "            \n",
    "    #creating the held-out set for interpolation\n",
    "    partitionFile('Data/LanguageModels/testing_corpus.txt',len(tokens))\n",
    "    \n",
    "    \n",
    "    with open('Data/LanguageModels/heldset_corpus.txt','r',encoding='latin1') as fheld:\n",
    "            hcontents=fheld.read()\n",
    "            htokens=contents.split()\n",
    "            htokenList=list(htokens)\n",
    "     \n",
    "    #Preprocessing the test set and held-out set\n",
    "    preprocess(tokens)\n",
    "    preprocess(htokens)\n",
    "    \n",
    "    bi=list(ngrams(tokenList2,2))\n",
    "    tri=list(ngrams(tokenList2,3))\n",
    "    quad=list(ngrams(tokenList2,4))\n",
    "    \n",
    "    hbi=list(ngrams(htokenList,2))\n",
    "    htri=list(ngrams(htokenList,3))\n",
    "    hquad=list(ngrams(htokenList,4))\n",
    "    \n",
    "    bigramSet2=set(bi)\n",
    "    trigramSet2=set(tri)\n",
    "    quadgramSet2=set(quad)\n",
    "    hod=Counter()\n",
    "    hod=unigramize(htokens)\n",
    "    hod2=Counter()\n",
    "    hod2=bigramize(hbi)\n",
    "    hod3=Counter()\n",
    "    hod3=trigramize(htri)\n",
    "    hod4=Counter()\n",
    "    hod4=quadgramize(hquad)\n",
    "   \n",
    "    \n",
    "    #Prediction of the word after interpolation using customised weights\n",
    "    #time1=time.time()\n",
    "    #quad_cpole_table=defaultdict(dict)\n",
    "    #l=[]\n",
    "    #quad_cpole_table=cus_interpolation_table(hod,hod2,hod3,hod4,l)\n",
    "    #word=quad_cpole_table[test_tri]\n",
    "    #word=sorted(word, key=lambda x: x[1], reverse=True)\n",
    "    #print (\"After interpolation, the most probable word could be: \")\n",
    "    #print (quad_cpole_table.items())\n",
    "    #print (word[0][0])\n",
    "    #print (l)\n",
    "    #print (\"Time taken for prediction: \", time.time()-time1)\n",
    "    \n",
    "    \n",
    "    #Calculating Score of the language model\n",
    "    time1=time.time()\n",
    "    print (\"Score of the language model is: \")\n",
    "    #print (scoreCalc(quad,tri,tokenList2))\n",
    "    print (\"Time taken for calculating Score: \", time.time()-time1)\n",
    "    \n",
    "    \n",
    "    #Add K Smoothing\n",
    "    smooth_tri=OrderedDict()\n",
    "    smooth_tri=OrderedDict.fromkeys(trigramSet2,0)\n",
    "    smooth_quad=OrderedDict()\n",
    "    smooth_quad=OrderedDict.fromkeys(quadgramSet2,0)\n",
    "    k=int(input(\"We will perform Add k smoothing now. Enter value of k: \"))\n",
    "    time1=time.time()\n",
    "    trismoothk(tokens,tri,trigramSet2,smooth_tri,k)\n",
    "    quadsmoothk(tokens,quad,quadgramSet2,smooth_quad,k)\n",
    "    print (\"Time taken for Add k Smoothing: \", time.time()-time1)\n",
    "    \n",
    "    #Good Turing Smoothing\n",
    "    od=unigramize(tokens) \n",
    "    wprob=defaultdict(dict)\n",
    "    time1=time.time()\n",
    "    wprob=goodturing(quadgramSet2,od4,od)\n",
    "    word=wprob[test_tri]\n",
    "    word=sorted(word, key=lambda x: x[1], reverse=True)\n",
    "    print (\"After Good Turing Smoothing, the most probable word could be: \")\n",
    "    print (word[0][0])\n",
    "    print (\"Time taken for Good Turing Smoothing: \", time.time()-time1)\n",
    "    \n",
    "    #Kneser Ney Smoothing\n",
    "    wprob=defaultdict(dict)\n",
    "    time1=time.time()\n",
    "    wprob=kney(od3,od4)\n",
    "    word=wprob[test_tri]\n",
    "    word=sorted(word, key=lambda x: x[1], reverse=True)\n",
    "    print (\"After Kneser Ney Smoothing, the most probable word could be: \")\n",
    "    print (word[0][0])\n",
    "    print (\"Time taken for Kneser Ney Smoothing: \", time.time()-time1)\n",
    "    \n",
    "    \n",
    "    #Perplexity\n",
    "    time1=time.time()\n",
    "    quadperp(tokenList2,smooth_quad,smooth_tri,quadgramSet2)\n",
    "    print (\"Time taken to compute perplexity: \", time.time()-time1)\n",
    "    \n",
    "    #Backoff\n",
    "    time1=time.time()\n",
    "    #backoff(tokens,quad,od4,od3,od2,od)\n",
    "    #print (\"Time taken to compute Backoff: \", time.time()-time1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your test string: I could not\n",
      "The next word could be: \n",
      "have\n",
      "Time taken for prediction:  0.39478206634521484\n",
      "After interpolation, the most probable word could be: \n",
      "be\n",
      "Time taken for prediction:  1.1483383178710938\n",
      "Score of the language model is: \n",
      "Time taken for calculating Score:  0.00020813941955566406\n",
      "We will perform Add k smoothing now. Enter value of k: 2\n",
      "Time taken for Add k Smoothing:  45.42597413063049\n",
      "After Good Turing Smoothing, the most probable word could be: \n",
      "hope\n",
      "Time taken for Good Turing Smoothing:  0.12499356269836426\n",
      "After Kneser Ney Smoothing, the most probable word could be: \n",
      "walk\n",
      "Time taken for Kneser Ney Smoothing:  0.9008536338806152\n",
      "Quadgram Perplexity = 1.052211\n",
      "Time taken to compute perplexity:  0.019231796264648438\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
