{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naive Baye's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding word counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to calculate the probabilities of each classification, and the probabilities of each feature falling into each classification.\n",
    "\n",
    "We were working with several discrete features in the last example. Here, all we have is one long string. The easiest way to generate features from text is to split the text up into words. Each word in a review will then be a feature that we can then work with. In order to do this, we’ll split the reviews based on whitespace.\n",
    "\n",
    "We’ll then count up how many times each word occurs in the negative reviews, and how many times each word occurs in the positive reviews. This will allow us to eventually compute the probabilities of a new review belonging to each class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative text sample: Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's bette\n",
      "Positive text sample: Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "i=0\n",
    "#print os.getcwd()\n",
    "posreviews=[]\n",
    "positive_text = \" \"\n",
    "negative_text = \" \"\n",
    "i=0\n",
    "for fileName in os.listdir(\"mr/train/pos\"):\n",
    "    fo=open(\"mr/train/pos/%s\" % fileName,\"r\")\n",
    "    #print fo.name\n",
    "    #os.rename(\"mr/train/neg/%s\" % fileName,\"mr/train/neg/%d.txt\" %(i))\n",
    "    str= fo.read()\n",
    "    #print \"%d. %s\" % (i,str)\n",
    "    posreviews.append(str)\n",
    "    #print \"%d . %s\" %(i,positive_text)\n",
    "    #if i>1000: \n",
    "     #   break\n",
    "    fo.close()\n",
    "    i=i+1\n",
    "positive_text= \" \".join(posreviews)\n",
    "#print positive_text\n",
    "negreviews=[]\n",
    "i=0\n",
    "for fileName in os.listdir(\"mr/train/neg\"):\n",
    "    fo=open(\"mr/train/neg/%s\" % fileName,\"r\")\n",
    "    #os.rename(\"mr/train/neg/%s\" % fileName,\"mr/train/neg/%d.txt\" %(i))\n",
    "    negreviews.append(fo.read())\n",
    "    #if i>1000: \n",
    "     #   break\n",
    "    i=i+1\n",
    "negative_text=\" \".join(negreviews)\n",
    "#print negative_text\n",
    "\n",
    "def count_text(text):\n",
    "  # Split text into words based on whitespace.  Simple but effective.\n",
    "  words = re.split(\"\\s+\", text)\n",
    "  # Count up the occurence of each word.\n",
    "  return Counter(words)\n",
    "\n",
    "# Generate word counts for negative tone.\n",
    "negative_counts = count_text(negative_text)\n",
    "# Generate word counts for positive tone.\n",
    "positive_counts = count_text(positive_text)\n",
    "\n",
    "print(\"Negative text sample: {0}\".format(negative_text[:500]))\n",
    "print(\"Positive text sample: {0}\".format(positive_text[:500]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the word counts, we just have to convert them to probabilities and multiply them out to get the predicted classification. Let’s say we wanted to find the probability that the review didn't like it expresses a negative sentiment. We would find the total number of times the word didn't occured in the negative reviews, and divide it by the total number of words in the negative reviews to get the probability of x given y. We would then do the same for like and it. We would multiply all three probabilities, and then multiply by the probability of any document expressing a negative sentiment to get our final probability that the sentence expresses negative sentiment.\n",
    "\n",
    "We would do the same for positive sentiment, and then whichever probability is greater would be the class that the review is assigned to.\n",
    "\n",
    "To do all this, we’ll need to compute the probabilities of each class occuring in the data, and then make a function to compute the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good nice Well Done - Positive (pos score=8.06432308243e-18, neg score=1.53669815111e-18)\n",
      "Movie was junk, useless, good for nothing, sheer waste of time and money. - Negative (pos score=6.35349543481e-48, neg score=2.56654331959e-45)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We need these counts to use for smoothing when computing the prediction.\n",
    "positive_review_count = len(posreviews)\n",
    "negative_review_count = len(negreviews)\n",
    "# These are the class probabilities (we saw them in the formula as P(y)).\n",
    "prob_positive = positive_review_count / float(len(posreviews) + len(negreviews))\n",
    "prob_negative = negative_review_count / float(len(posreviews) + len(negreviews))\n",
    "\n",
    "def make_class_prediction(text, counts, class_prob, class_count):\n",
    "  prediction = 1.0\n",
    "  text_counts = Counter(re.split(\"\\s+\", text))\n",
    "  for word in text_counts:\n",
    "      # For every word in the text, we get the number of times that word occured in the reviews for a given class, add 1 to smooth the value, and divide by the total number of words in the class (plus the class_count to also smooth the denominator).\n",
    "      # Smoothing ensures that we don't multiply the prediction by 0 if the word didn't exist in the training data.\n",
    "      # We also smooth the denominator counts to keep things even.\n",
    "      # print \"%d, %d,%d\" %(text_counts.get(word), counts.get(word,0), sum(counts.values()))\n",
    "      prediction *=  text_counts.get(word) * ((counts.get(word,0) + 1) / float(sum(counts.values()) + class_count))\n",
    "      #print prediction\n",
    "  # Now we multiply by the probability of the class existing in the documents.\n",
    "  return prediction * class_prob\n",
    "\n",
    "# As you can see, we can now generate probabilities for which class a given review is part of.\n",
    "# The probabilities themselves aren't very useful -- we make our classification decision based on which value is greater.\n",
    "#print(\"Review: {0}\".format(reviews[0][0]))\n",
    "#text=\"Movie was junk, useless, good for nothing, sheer waste of time and money.\"\n",
    "text=\"Good nice Well Done\"\n",
    "#print(\"Negative prediction: {0}\".format(make_class_prediction(text, negative_counts, prob_negative, negative_review_count))\n",
    "neg=make_class_prediction(text, negative_counts, prob_negative, negative_review_count)\n",
    "pos=make_class_prediction(text, positive_counts, prob_positive, positive_review_count)\n",
    "if pos>neg:\n",
    "   print \"%s - Positive (pos score={0}, neg score={1})\".format(pos,neg) %(text)\n",
    "else:\n",
    "   print \"%s - Negative (pos score={0}, neg score={1})\".format(pos,neg) %(text)\n",
    "text=\"Movie was junk, useless, good for nothing, sheer waste of time and money.\"\n",
    "neg=make_class_prediction(text, negative_counts, prob_negative, negative_review_count)\n",
    "pos=make_class_prediction(text, positive_counts, prob_positive, positive_review_count)\n",
    "if pos>neg:\n",
    "   print \"%s - Positive (pos score={0}, neg score={1})\".format(pos,neg) %(text)\n",
    "else:\n",
    "   print \"%s - Negative (pos score={0}, neg score={1})\".format(pos,neg) %(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Faster way to predict using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial naive bayes AUC: 0.785085965614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "test=[]\n",
    "actual=[]\n",
    "for fileName in os.listdir(\"mr/test/pos\"):\n",
    "    fo=open(\"mr/test/pos/%s\" % fileName,\"r\")\n",
    "    str= fo.read()\n",
    "    test.append(str)\n",
    "    actual.append(1)\n",
    "for fileName in os.listdir(\"mr/test/neg\"):\n",
    "    fo=open(\"mr/test/neg/%s\" % fileName,\"r\")\n",
    "    str= fo.read()\n",
    "    test.append(str)\n",
    "    actual.append(-1)\n",
    "reviews = []\n",
    "for r in posreviews:\n",
    "    reviews.append(r)\n",
    "for r in negreviews:\n",
    "    reviews.append(r)\n",
    "# Generate counts from text using a vectorizer.  There are other vectorizers available, and lots of options you can set.\n",
    "# This performs our step of computing word counts.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_features = vectorizer.fit_transform([r for r in reviews])\n",
    "test_features = vectorizer.transform([r for r in test])\n",
    "\n",
    "# Fit a naive bayes model to the training data.\n",
    "# This will train the model using the word counts we computer, and the existing classifications in the training set.\n",
    "nb = MultinomialNB()\n",
    "trainRes=[]\n",
    "for r in posreviews:\n",
    "    trainRes.append(1)\n",
    "for r in negreviews:\n",
    "    trainRes.append(-1)\n",
    "nb.fit(train_features,trainRes)\n",
    "\n",
    "# Now we can use the model to predict classifications for our test features.\n",
    "predictions = nb.predict(test_features)\n",
    "#print predictions\n",
    "# Compute the error.  It is slightly different from our model because the internals of this process work differently from our implementation.\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "print(\"Multinomial naive bayes AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using SVM\n",
    "First Sample Kernal is rbf(radial basis fuction) i.e. exp(-gamma |x-x'|^2) where gamma is a specified variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ..., -1 -1 -1]\n",
      "SVC Analysis AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_features, trainRes) \n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "predictSvc=clf.predict(test_features)\n",
    "print predictSvc\n",
    "# Compute the error.  \n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictSvc, pos_label=1)\n",
    "print(\"SVC Analysis AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernal as Linear Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ..., -1 -1 -1]\n",
      "SVC Analysis AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "predictSvc=clf.predict(test_features)\n",
    "print predictSvc\n",
    "# Compute the error.  \n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictSvc, pos_label=1)\n",
    "print(\"SVC Analysis AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernal as Sigmoid function i.e. tanh(gamma* < x, x'> + r) where r is specified by variable coef0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ..., -1 -1 -1]\n",
      "SVC Analysis AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=200.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='sigmoid',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "predictSvc=clf.predict(test_features)\n",
    "print predictSvc\n",
    "# Compute the error.  \n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictSvc, pos_label=1)\n",
    "print(\"SVC Analysis AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Trees 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ..., -1 -1 -1]\n",
      "SVC Analysis AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                              min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                              max_leaf_nodes=None,bootstrap=True, oob_score=False, \n",
    "                                              n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "predictRF=clf.predict(test_features)\n",
    "print predictSvc\n",
    "# Compute the error.  \n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictSvc, pos_label=1)\n",
    "print(\"SVC Analysis AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Trees 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ..., -1 -1 -1]\n",
      "SVC Analysis AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                              min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                              max_leaf_nodes=None, bootstrap=True, oob_score=False, \n",
    "                                              n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "predictRF=clf.predict(test_features)\n",
    "print predictSvc\n",
    "# Compute the error.  \n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictSvc, pos_label=1)\n",
    "print(\"SVC Analysis AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "Dataset - http://ai.stanford.edu/~amaas/data/sentiment/ [25k(12.5k pos and 12.5k negative reviews) Training Data and 25k Test Movie Reviews] <br/>We took 2.5k pos samples and neg samples from test and tag for the results which are displayed.<br/>Kindly download the dataset from the link provided and make sure it is in the same folder as this ipython notebook to run.<br/>\n",
    "Naive Bayes Senti Analysis on Movie Reviews Blog https://www.dataquest.io/blog/naive-bayes-tutorial/ <br/>\n",
    "Developed by Mayank Bhasin, for any queries contact mayankbhasin@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
