{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction using n gram model\n",
    "    (Using an OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import OrderedDict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the training corpus to convert to lowercase and remove punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tokenList):\n",
    "    i=0\n",
    "    for word1 in tokenList:\n",
    "    #conversion into lowercase\n",
    "        word1=word1.lower()\n",
    "    #Takes Care of Multiple Punctuation Marks\n",
    "        word1=word1.replace('.','').replace(',','').replace(':','').replace(';','').replace('!','').replace('?','').replace('(','').replace(')','').replace('-','').replace('_','').replace('\\\\',' ').replace('\\\"',' ').replace('\\'',' ')      \n",
    "    \n",
    "        tokenList[i]=word1        \n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is being split into an ordered dictionary of unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigramize(unigrams):\n",
    "    \n",
    "    od=Counter()\n",
    "    \n",
    "    for item in unigrams:\n",
    "        od[item]+=1\n",
    "        \n",
    "    return od"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is being split into an ordered dictionary of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigramize(bigrams):\n",
    "    od2=Counter()\n",
    "\n",
    "\n",
    "    for item in bigrams:\n",
    "        od2[item]+=1\n",
    "        \n",
    "    return od2\n",
    "            \n",
    "    #print (od3.keys())\n",
    "    #print (od3.values())\n",
    "    #print (len(od3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is being split into an ordered dictionary of trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigramize(trigrams):\n",
    "    od3=Counter()\n",
    "\n",
    "\n",
    "    for item in trigrams:\n",
    "        od3[item]+=1\n",
    "        \n",
    "    return od3\n",
    "            \n",
    "    #print (od3.keys())\n",
    "    #print (od3.values())\n",
    "    #print (len(od3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is being split into an ordered dictionary of quadgrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadgramize(quadgrams):\n",
    "    od4=Counter()\n",
    "\n",
    "    for item in quadgrams:\n",
    "        od4[item]+=1\n",
    "        \n",
    "    return od4\n",
    "            \n",
    "    #print (od4.keys())\n",
    "    #print (od4.values())\n",
    "    #print (len(od4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the probability table for the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probtable(od3,od4,trigramSet):\n",
    "    \n",
    "    max_freq=0\n",
    "    freq=0\n",
    "\n",
    "\n",
    "    prob=OrderedDict()\n",
    "    prob=OrderedDict.fromkeys(quadgramSet,0)\n",
    "\n",
    "    for item,value in od4.items():\n",
    "        for item2,value2 in od3.items():\n",
    "            #print (item)\n",
    "            if(item2 in item):\n",
    "                prob[item]=value2/value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the N-th probable word which might occur after our input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(od4,test,token,n):\n",
    "    \n",
    "    pred=OrderedDict()\n",
    "    pred=OrderedDict.fromkeys(token,0)\n",
    "\n",
    "    for item,value in od4.items():\n",
    "        out=\" \".join([str(s) for s in list(item)])\n",
    "        if not(out.find(test)):\n",
    "            pred[item[3]]+=(1)\n",
    "\n",
    "        \n",
    "    OrderedDict(sorted(od4.items(), key=lambda t: t[1],reverse=True))        \n",
    "    #v=list(pred.values())\n",
    "    #k=list(pred.keys())\n",
    "    \n",
    "        \n",
    "    #print (pred.items())  \n",
    "    #return (k[v.index(max(v))])\n",
    "    return list(pred.keys())[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def interpolation(test,token,od,od2,od3,od4):\n",
    "    \n",
    "    pole=OrderedDict()\n",
    "    pole=OrderedDict.fromkeys(token,0)\n",
    "    \n",
    "    i=0\n",
    "    s=0.0\n",
    "    lambda1=0.3\n",
    "    lambda2=0.3\n",
    "    lambda3=0.3\n",
    "    \n",
    "    for item,value in od4.items():\n",
    "        #if i>100:\n",
    "         #   break\n",
    "        out=\" \".join([str(s) for s in list(item)])\n",
    "        if (out.find(test)):\n",
    "            pole[item[3]]=lambda1*float((value)/od3[item[0:3]]) + lambda2*float((od3[item[1:4]])/(od2[item[1:3]])) + lambda3*float((od2[item[2:4]])/(od[item[2]]+1)) \n",
    "        #print (pole[item[3]])\n",
    "        #i=i+1\n",
    "        \n",
    "    #v=list(pole.values())\n",
    "    #k=list(pole.keys())\n",
    "    \n",
    "        \n",
    "    #print (pred.items())  \n",
    "    #return (k[v.index(max(v))])\n",
    "        \n",
    "    #print (pole.keys())\n",
    "    #print (pole.values())\n",
    "    \n",
    "    return pole\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score of the language model\n",
    "\n",
    "Score of the language model is a yardstick to measure how good our model is at predicting words based on the corpus it has been trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scoreCalc(quad,tri,tokenList2):\n",
    "    score=0\n",
    "    scorepred=OrderedDict()\n",
    "    scorepred=OrderedDict.fromkeys(tokenList2,0)\n",
    "\n",
    "    for item in quad:\n",
    "        if item[0:3] in tri:\n",
    "            scorepred[item[3]]+=1\n",
    "        v=list(scorepred.values())\n",
    "        k=list(scorepred.keys())\n",
    "        if (k[v.index(max(v))]==item[3]):\n",
    "            score+=1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Smoothing\n",
    "\n",
    "    Here we apply Add 1 smoothing to bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bismooth(tokens,bi,bigramSet2,smooth_bi):\n",
    "    \n",
    "    #Add 1 Smoothing for bigram model\n",
    "    #smooth_bi=OrderedDict()\n",
    "    #smooth_bi=OrderedDict.fromkeys(bigramSet2,0)\n",
    "    i=0\n",
    "    \n",
    "    for item in bigramSet2:\n",
    "        #smooth_bi.append(bi.count(item) + 1/float((token.count(item[0])+ len(bigramSet2))))\n",
    "        smooth_bi[item]=bi.count(item) + 1/float((tokens.count(item[0])+ len(bigramSet2)))\n",
    "        #print (\"%s ->  %f\" %(item,counting[i]/float((tokens.count(item[0])+ len(bigramSet2)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 1 smoothing to trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trismooth(tokens,tri,trigramSet2,smooth_tri):\n",
    "    \n",
    "    #Add 1 Smoothing for trigram model\n",
    "    #smooth_tri=OrderedDict()\n",
    "    #smooth_tri=OrderedDict.fromkeys(trigramSet2,0)\n",
    "    i=0\n",
    "    \n",
    "    for item in trigramSet2:\n",
    "        #smooth_bi.append(bi.count(item) + 1/float((token.count(item[0])+ len(bigramSet2))))\n",
    "        smooth_tri[item]=tri.count(item) + 1/float((tokens.count(item[0:2])+ len(trigramSet2)))\n",
    "        #print (\"%s ->  %f\" %(item,counting[i]/float((tokens.count(item[0])+ len(bigramSet2)))))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 1 smoothing to quadgram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quadsmooth(tokens,quad,quadgramSet2,smooth_quad):\n",
    "    \n",
    "    #Add 1 Smoothing for quadgram model\n",
    "    #smooth_quad=OrderedDict()\n",
    "    #smooth_quad=OrderedDict.fromkeys(quadgramSet2,0)\n",
    "    i=0\n",
    "    \n",
    "    for item in quadgramSet2:\n",
    "        #smooth_bi.append(bi.count(item) + 1/float((token.count(item[0])+ len(bigramSet2))))\n",
    "        smooth_quad[item]=quad.count(item) + 1/float((tokens.count(item[0:3])+ len(quadgramSet2)))\n",
    "        #print (\"%s ->  %f\" %(item,counting[i]/float((tokens.count(item[0])+ len(bigramSet2)))))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "    Unigram perplexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniperp(tokenList2):\n",
    "    #computing unigram perplexity\n",
    "    perplexity1=float(1.0)\n",
    "    n= len (tokenList2)\n",
    "    i=0\n",
    "    for item in tokenList2:\n",
    "        perplexity1=perplexity1*((1/float(tokenList2.count(item)))**(1./n))\n",
    "    perplexity1=(perplexity1)*len(tokenList2)\n",
    "    print (\"Unigram perplexity = %f\" %(perplexity1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Bigram perplexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def biperp(tokenList2,smooth_bi,bigramSet2):\n",
    "    \n",
    "    #computing bigram perplexity\n",
    "    perplexity2=1.0\n",
    "    i=0\n",
    "    k=0\n",
    "    n= len (tokenList2)\n",
    "    for item in bigramSet2:\n",
    "        #perplexity2= perplexity2*(((1/float(bi.count(item)))*tokenList2.count(item[0]))**(1./n))\n",
    "        perplexity2= perplexity2*(((1/float(smooth_bi[item]))*tokenList2.count(item[0]))**(1./n))\n",
    "        #print (\"%f\" %(tokenList2.count(item[0])))\n",
    "    print (\"Bigram perplexity = %f\" %(perplexity2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Trigram Perplexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def triperp(tokenList2,smooth_tri,smooth_bi,trigramSet2):\n",
    "    \n",
    "    #computing trigram perplexity\n",
    "    n= len (tokenList2)\n",
    "    perplexity3=1.0\n",
    "    i=0\n",
    "    for item in trigramSet2:\n",
    "           perplexity3=perplexity3*(((1/float(smooth_tri[item]))*smooth_bi[item[0:2]])**(1./n))\n",
    "           \n",
    "    #perplexity3 = perplexity3)) * ((n+k)/n)\n",
    "    print (\"Trigram perplexity = %f\" %(perplexity3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Quadgram Perplexity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadperp(tokenList2,smooth_quad,smooth_tri,quadgramSet2):\n",
    "    \n",
    "    #computing quadgram perplexity\n",
    "    n= len (tokenList2)\n",
    "    perplexity4=1.0\n",
    "    i=0\n",
    "    for item in quadgramSet2:\n",
    "        perplexity4=perplexity4*(((1/float(smooth_quad[item]))*smooth_tri[item[0:3]])**(1./n))\n",
    "        i=i+1\n",
    "    print (\"Quadgram Perplexity = %f\" %(perplexity4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to call all modules respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    f=open('Data/LanguageModels/training_corpus.txt','r',encoding='latin1')\n",
    "    content=f.read()\n",
    "    #token=nltk.word_tokenize(nltk.corpus.gutenberg.raw(nltk.corpus.gutenberg.fileids()[0]))\n",
    "    #content=\"I like to play likKe . ;to sleep? football in the rain with my shoes on my shoes on and dancing in the rain with blah\"\n",
    "    token=content.split()\n",
    "\n",
    "    #splitting into tokens\n",
    "    tokenList=list(token)\n",
    "    tokenSet=set(tokenList)\n",
    "    #print (\"Length of token list: \"+ str(len(token)))\n",
    "\n",
    "    #storing stop words separately\n",
    "    stop = set(stopwords.words('english'))\n",
    "    #print (len(stop))\n",
    "    #tokenSet=tokenSet-stop\n",
    "\n",
    "\n",
    "    #listing the tokens into n-grams\n",
    "    unigrams=list(ngrams(token,1))\n",
    "    bigrams=list(ngrams(token,2))\n",
    "    trigrams=list(ngrams(token,3))\n",
    "    quadgrams=list(ngrams(token,4))\n",
    "    pentgrams=list(ngrams(token,5))\n",
    "    bigramSet=set(bigrams)\n",
    "    trigramSet=set(trigrams)\n",
    "    quadgramSet=set(quadgrams)\n",
    "    pentgramSet=set(pentgrams)\n",
    "\n",
    "    #creating set of unique words\n",
    "\n",
    "    #print (\"Number of unique words in test set: \"+str(len(tokenSet)))\n",
    "    \n",
    "    preprocess(tokenList)\n",
    "    \n",
    "    od=Counter()\n",
    "    od=unigramize(unigrams)\n",
    "    #print (od.values())\n",
    "    \n",
    "    od2=Counter()\n",
    "    od2=bigramize(bigrams)\n",
    "    \n",
    "    od3=Counter()\n",
    "    \n",
    "    od3=trigramize(trigrams)\n",
    "    \n",
    "    od4=Counter()\n",
    "    \n",
    "    od4=quadgramize(quadgrams)\n",
    "    \n",
    "    #Taking our input string\n",
    "    sent=input(\"Enter your test string: \")\n",
    "    list2=sent.split()\n",
    "    test=' '.join(list2[(len(list2)-3):len(list2)])\n",
    "    \n",
    "    del(list2)\n",
    "    \n",
    "    #print (\"We will find the N-th probable word which can occur after the test string. \")\n",
    "    #n_prob=int(input(\"Please enter N:\"))\n",
    "    \n",
    "    print (\"The next word could be: \")\n",
    "    #print (prediction(od4,test,token,1))\n",
    "    \n",
    "    \n",
    "    #computing score of the language model\n",
    "    score=0\n",
    "    with open('Data/LanguageModels/testing_corpus.txt','r',encoding='latin1') as f:\n",
    "            contents=f.read()\n",
    "            tokens=contents.split()\n",
    "            tokenList2=list(tokens)\n",
    "            \n",
    "    preprocess(tokens)\n",
    "    \n",
    "    bi=list(ngrams(tokenList2,2))\n",
    "    tri=list(ngrams(tokenList2,3))\n",
    "    quad=list(ngrams(tokenList2,4))\n",
    "    \n",
    "    #print (\"Score of the language model is: \")\n",
    "    #print (scoreCalc(quad,tri,tokenList2))\n",
    "    \n",
    "    \n",
    "    bigramSet2=set(bi)\n",
    "    trigramSet2=set(tri)\n",
    "    quadgramSet2=set(quad)\n",
    "    \n",
    "    smooth_bi=OrderedDict()\n",
    "    smooth_bi=OrderedDict.fromkeys(bigramSet2,0)\n",
    "    #bismooth(tokens,bi,bigramSet2,smooth_bi)\n",
    "    \n",
    "    smooth_tri=OrderedDict()\n",
    "    smooth_tri=OrderedDict.fromkeys(trigramSet2,0)\n",
    "    #trismooth(tokens,tri,trigramSet2,smooth_tri)\n",
    "    \n",
    "    smooth_quad=OrderedDict()\n",
    "    smooth_quad=OrderedDict.fromkeys(quadgramSet2,0)\n",
    "    #quadsmooth(tokens,quad,quadgramSet2,smooth_quad)\n",
    "    \n",
    "    #uniperp(tokenList2)\n",
    "    \n",
    "    #biperp(tokenList2,smooth_bi,bigramSet2)\n",
    "    \n",
    "    #triperp(tokenList2,smooth_tri,smooth_bi,trigramSet2)\n",
    "    \n",
    "    #quadperp(tokenList2,smooth_quad,smooth_tri,quadgramSet2)\n",
    "\n",
    "    pole=OrderedDict()\n",
    "    pole=interpolation(test,token,od,od2,od3,od4)\n",
    "    v=list(pole.values())\n",
    "    k=list(pole.keys())\n",
    "    print (k[v.index(max(v))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your test string: Neither you nor I\n",
      "The next word could be: \n",
      "am\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
